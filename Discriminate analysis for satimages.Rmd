---
title: "Discriminant Analysis of SATELLITE IMAGE DATA"
author: "Masha"
date: "5/13/2020"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Libraries needed
```{r}
library(klaR)
library(MASS)
```

Importing the Data
```{r}
SATimage =read.csv("C:/Users/fb8502oa/Desktop/Github stuff/Discriminant-Analysis-R-/SATimage (assign8).csv")  
SATimage = data.frame(class=as.factor(SATimage$class),SATimage[,1:36])

```

forming training and test sets
```{r}
set.seed(888)# to have the same data
testcases = sample(1:dim(SATimage)[1],1000,replace=F)
SATtest = SATimage[testcases,]
SATtrain = SATimage[-testcases,]
```

looking at the dimensions 
```{r}
dim(SATtrain)
dim(SATtest)
```

making sure the test and training are fine 
```{r}
table(SATtrain$class)
table(SATtest$class)
```

FIRST MODEL LDA
```{r}
SAT.lda = lda(class~., data = SATtrain)
summary(SAT.lda)
```

```{r}
yfit = predict(SAT.lda, newdata = SATtrain)
attributes(yfit)
```


Misclassification function (prof: Brant Deppas's function)
```{r}
misclass = function(fit,y) {
temp <- table(fit,y)
cat("Table of Misclassification\n")
cat("(row = predicted, col = actual)\n")
print(temp)
cat("\n\n")
numcor <- sum(diag(temp))
numinc <- length(y) - numcor
mcr <- numinc/length(y)
cat(paste("Misclassification Rate = ",format(mcr,digits=3)))
cat("\n")
}
```


Misclassification for the training set 
```{r}
misclass(yfit$class, SATtrain$class)
```

ploting for two dimensions of
```{r}
plot(yfit$x[,1], yfit$x[,2], type = "n",xlab = "First Discriminant", ylab= "Second Discriminant", main = "D2 vs D1")
text(yfit$x[,1], yfit$x[,2], as.character(yfit$class), col = as.numeric(yfit$class)+1, cex = 0.9)
```

scatterplot function
```{r}
pairs.grps = function(x) {
  pairs(x[,-1],pch=21,bg=as.numeric(as.factor(x[,1]))+3)
}
```

scatter plots 
```{r}
poo = cbind(SATtrain$class,yfit$x)
pairs.grps(poo)
```

Misclassiffication of the test set.
```{r}
ypred= predict(SAT.lda, newdata = SATtest)
misclass(ypred$class, SATtest$class)
```


SUMMARY:

After splitting the data into a training and a test set, I fit the LDA model with the training set. It had a
misclassification rate of 0.151 in the training set and a misclassification rate of 0.15 in the testing set



##########

MODEL 2 QDA 
```{r}
SAT.qda = qda(class~.,data=SATtrain)
yfit = predict(SAT.qda,newdata=SATtrain)
attributes(yfit)
misclass(yfit$class,SATtrain$class)

```

Now on the test cases 
```{r}
ypred = predict(SAT.qda,newdata=SATtest)
misclass(ypred$class, SATtest$class)
```


SUMMARY:
I used the training data to fit a QDA model which had a misclassification rate of 0.107 in the training set
and a misclassification rate of 0.135 in the test set. Both the training and test set performed better than
the LDA modelâ€™s.



##########

MODEL 3 RDA using klaR lib with function rda()
```{r}
library(klaR)
SAT.rda = rda(class~., data = SATtrain)
attributes(SAT.rda)
```

looking for optimal settings 
```{r}
SAT.rda$regularization
```

```{r}
SAT.rda$error.rate
```

Now looking at the test cases
```{r}
ypred = predict(SAT.rda, newdata = SATtest)
misclass(ypred$class, SATtest$class)
```

TUNING PARAMETERS FOR THE RDA MODEL 
```{r}
SAT.rda2 = rda(class~., data = SATtrain, lambda = 0.70, gamma =0.002)
pred.rda2 = predict(SAT.rda2, newdata = SATtest)
misclass(pred.rda2$class, SATtest$class)
```

SUMMARY:

I used the klaR library to fit a RDA model and it had a gamma of 0.00208and a lambda of 0.74130 as its
optimal setting for 10 folds. The estimated error rate from 10 folds cv results was 0.11558 to after cross -
validation 0.14062.
This model had a misclassification rate of 0.1406 on the training set and a misclassification rate of 0.129
on the test set.


tuning parameter function (prof: Brant Deppa's function)
```{r}
find.gamlam = function(formula,train,test,ming=0,maxg=1,minl=0,maxl=1,k=5){
    lambda = seq(minl,maxl,length=k)
    gamma = seq(ming,maxg,length=k)
    mcr = rep(0,as.integer(k^2))
    ntest = dim(test)[1]
    lg.grid = expand.grid(lambda=lambda,gamma=gamma)
    for (i in 1:as.integer(k^2)){
        temp = rda(formula,data=train,lambda=lg.grid[i,1],gamma=lg.grid[i,2])
        pred = predict(temp,newdata=test)$class
        numinc = ntest - sum(diag(table(pred,test[,1])))
        mcr[i] = numinc/ntest
    }
    cbind(lg.grid,mcr)
}
```

finding the tuning parameters 
```{r}
find.gamlam(class~., train = SATtrain, test = SATtest, k =10)
```

```{r}
find.gamlam(class~., train = SATtrain, test = SATtest, k =10, minl = 0.40, maxl = 0.56, ming = 0, maxg = 0.001)
```

SUMMARY:
Using find.gamlam function, I fit a model with 10 k_fold validations, a min lambda of 0.40, a max lambda
of 0.56, a min gamma of 0 and a max gamma of 0.001. The optimal lambda was 0.542 and the optimal
gamma was 0.0


LOKKING AT THE FINAL RDA MODEL  
```{r}
SAT.rda3 = rda(class~., data = SATtrain, gamma= 0.0, lambda = 0.542)
yfti = predict(SAT.rda3, newdata = SATtrain)
misclass(yfit$class, SATtrain$class)
```

looking at its test cases 
```{r}
ypred = predict(SAT.rda3,newdata = SATtest)
misclass(ypred$class, SATtest$class)
```

SUMMARY:

I got the best model after fitting it with the optimal lambda and gamma. It had a misclassification rate of
0.107 on the training set and a misclassification rate of 0.125 on the test set.

